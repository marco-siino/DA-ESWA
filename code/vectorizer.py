# -*- coding: utf-8 -*-
"""vectorizer.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3xu1F7Slt8DgUjv4bJw1S5VOInfDh4S
"""

import tensorflow as tf

import numpy as np

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from keras.models import Model
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

class Vectorizer:

  # Preprocessing function to remove some noise due to the translation.
  def clean_samples(self,input_data):
    tag_author_lang_en_removed = tf.strings.regex_replace(input_data,'<author lang="en">', '')
    tag_opening_document_miscased = tf.strings.regex_replace(tag_author_lang_en_removed,'<Document>', '<document>')
    tag_closing_document_miscased = tf.strings.regex_replace(tag_opening_document_miscased,'</Document>', '</document>')
    tag_opening_documents = tf.strings.regex_replace(tag_closing_document_miscased,'<documents>', '')
    tag_opening_cdata_removed = tf.strings.regex_replace(tag_opening_documents,'<\!\[CDATA\[', ' ')
    tag_closing_cdata_removed = tf.strings.regex_replace(tag_opening_cdata_removed,'\]\]>', ' >')
    output_data = tf.strings.regex_replace(tag_closing_cdata_removed,'</documents>', '')
    return output_data

  def __init__(self, train_set):
    # Set a large sequence length to find the longest sample in the training set.
    sequence_length = 200000
    vectorize_layer = TextVectorization(
        standardize=self.clean_samples,
        output_mode='int',
        output_sequence_length=sequence_length)

    train_text = train_set.map(lambda x, y: x)
    vectorize_layer.adapt(train_text)
    #vectorize_layer.get_vocabulary()

    model = tf.keras.models.Sequential()
    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    model.add(vectorize_layer)

    longest_sample_length=1

    for element in train_set:
      authorDocument=element[0]
      label=element[1]
      
      #print("Sample considered is: ", authorDocument[0].numpy())
      #print("Preprocessed: ", str(custom_standardization(authorDocument[0].numpy())))
      #print("And has label: ", label[0].numpy())

      out=model(authorDocument)
      # Convert token list to numpy array.
      token_list = out.numpy()[0]
      token_list = np.trim_zeros(token_list,'b')
      if longest_sample_length < len(token_list):
        longest_sample_length = len(token_list)

    print("Length of the longest sample is:", longest_sample_length)

    # After tokenization longest_sample_length covers all the document lenghts in our dataset.
    sequence_length = longest_sample_length

    vectorize_layer = TextVectorization(
        standardize=self.clean_samples,
        output_mode='int',
        output_sequence_length=sequence_length)

    # Finally adapt the vectorize layer.
    train_text = train_set.map(lambda x, y: x)
    vectorize_layer.adapt(train_text)
    max_features=len(vectorize_layer.get_vocabulary()) + 1
    print("\nVocabulary size is:", max_features)
    self.vectorize_layer = vectorize_layer